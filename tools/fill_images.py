import result
import requests
from bs4 import BeautifulSoup
import json
import time
import random
import urllib.parse
import re

# result.pyì—ì„œ SEARCH_URLS ê°€ì ¸ì˜¤ê¸°
SEARCH_URLS = result.SEARCH_URLS

def get_image_from_url(url, is_search=False):
    """URLì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ (ìƒì„¸í˜ì´ì§€ og:image ë˜ëŠ” ê²€ìƒ‰ê²°ê³¼ ì²«ë²ˆì§¸ ì´ë¯¸ì§€)"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Referer': 'https://www.google.com/'
        }
        
        response = requests.get(url, headers=headers, timeout=5)
        if response.status_code != 200:
            return None
            
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # 1. ìƒì„¸ í˜ì´ì§€ì¼ ê²½ìš° (og:image)
        if not is_search:
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"):
                return normalize_url(og_image["content"], url)
        
        # 2. ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì¼ ê²½ìš° (ì²«ë²ˆì§¸ ìƒí’ˆ ì´ë¯¸ì§€)
        else:
            # Cafe24 ì¼ë°˜ì ì¸ êµ¬ì¡° (prdList, thumbnail)
            # ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„
            selectors = [
                ".prdList .thumb img",      # ì¼ë°˜ì ì¸ ëª©ë¡
                ".thumbnail img",           # ì¸ë„¤ì¼ í´ë˜ìŠ¤
                ".prdImg img",              # ìƒí’ˆ ì´ë¯¸ì§€ í´ë˜ìŠ¤
                ".ec-base-product .thumb img" 
            ]
            
            for sel in selectors:
                img = soup.select_one(sel)
                if img and img.get('src'):
                    src = img['src']
                    # ec-img-hover ê°™ì€ê±° ë§ê³  ë©”ì¸ ì´ë¯¸ì§€
                    return normalize_url(src, url)
                    
        return None
        
    except Exception as e:
        # print(f"Error scraping {url}: {e}")
        return None

def normalize_url(img_url, base_url):
    """ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜"""
    if img_url.startswith("//"):
        return "https:" + img_url
    if img_url.startswith("/"):
        # base_urlì˜ ë„ë©”ì¸ ì¶”ì¶œ
        parsed = urllib.parse.urlparse(base_url)
        return f"{parsed.scheme}://{parsed.netloc}{img_url}"
    if not img_url.startswith("http"):
        # ê²½ë¡œê°€ ì¢€ ì´ìƒí•˜ë©´ ì¼ë‹¨ í•©ì¹˜ê¸° ì‹œë„
        parsed = urllib.parse.urlparse(base_url)
        return f"{parsed.scheme}://{parsed.netloc}/{img_url}"
    return img_url

def fill_images():
    print("ë°ì´í„° ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
    data, _ = result.process_data()
    
    # 2ê³³ ì´ìƒ íŒë§¤ì¤‘ì¸ë° ì´ë¯¸ì§€ê°€ ì—†ëŠ” ìƒí’ˆ í•„í„°ë§
    targets = []
    for key, item in data.items():
        if not item.get('image') and len(item['prices']) >= 2:
            targets.append((key, item))
            
    print(f"ğŸ¯ ì´ {len(targets)}ê°œì˜ ìƒí’ˆì— ëŒ€í•´ ì´ë¯¸ì§€ ê²€ìƒ‰ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    found_images = {}
    count = 0
    success = 0
    
    # ìš°ì„ ìˆœìœ„ ì‚¬ì´íŠ¸ (ê²€ìƒ‰ì´ ì˜ ë˜ëŠ” ê³³)
    PRIORITY_SITES = ['modu', 'juice24', 'tjf']

    for key, item in targets:
        count += 1
        print(f"[{count}/{len(targets)}] {item['display_name']} ê²€ìƒ‰ ì¤‘...", end="\r")
        
        # 1. ê¸°ì¡´ ë§í¬ í™•ì¸
        links = [site_info['link'] for site_info in item['prices'].values() if site_info.get('link')]
        valid_links = [l for l in links if "search.html" not in l]
        
        img_url = None
        
        # 1-1. ìƒì„¸ í˜ì´ì§€ ë§í¬ê°€ ìˆìœ¼ë©´ ê±°ê¸°ì„œ ì‹œë„
        for link in valid_links:
            img_url = get_image_from_url(link, is_search=False)
            if img_url and check_valid_image(img_url): break
            time.sleep(random.uniform(0.5, 1.0))
            
        # 2. ë§í¬ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨í–ˆìœ¼ë©´ 'ê²€ìƒ‰' ì‹œë„
        if not img_url:
            # ê²€ìƒ‰ì–´ ìƒì„± (íŠ¹ìˆ˜ë¬¸ì ì œê±° ë“±)
            query = clean_query(item['display_name'])
            encoded_query = urllib.parse.quote(query)
            
            # ìš°ì„ ìˆœìœ„ ì‚¬ì´íŠ¸ ìˆœíšŒ
            for site in PRIORITY_SITES:
                if site not in SEARCH_URLS: continue
                
                search_prefix = SEARCH_URLS[site]
                search_url = f"{search_prefix}{encoded_query}"
                
                # print(f"  - ê²€ìƒ‰ ì‹œë„: {site}")
                img_url = get_image_from_url(search_url, is_search=True)
                
                if img_url and check_valid_image(img_url):
                    # print(f"  -> ê²€ìƒ‰ ì„±ê³µ: {img_url}")
                    break
                    
                time.sleep(random.uniform(1.0, 1.5))
        
        if img_url:
            found_images[key] = img_url
            success += 1
            
    print(f"\nâœ¨ ì™„ë£Œ! {success}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì €ì¥
    with open("additional_images.json", "w", encoding="utf-8") as f:
        json.dump(found_images, f, ensure_ascii=False, indent=2)

def check_valid_image(url):
    if not url: return False
    if "placeholder" in url: return False
    if "noimg" in url: return False
    if "btn_buy" in url: return False # ê°„í˜¹ ë²„íŠ¼ ì´ë¯¸ì§€ê°€ ì¡í ë•Œ
    return True

def clean_query(name):
    # ê²€ìƒ‰ ì •í™•ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ [] ê´„í˜¸ ì œê±° ë“±
    name = re.sub(r'\[.*?\]', '', name)
    name = re.sub(r'\(.*?\)', '', name)
    # ìš©ëŸ‰ ì œê±° (30ml ë“±)
    name = re.sub(r'\d+ml', '', name, flags=re.IGNORECASE)
    return name.strip()

if __name__ == "__main__":
    fill_images()
